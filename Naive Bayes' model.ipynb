{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9bd69e1",
   "metadata": {},
   "source": [
    "# What this document is\n",
    "This document implements a basic Naive Bayes model to classify spam sms or similar spam email. This article will not go deep into the math and mostly is an implementation guide.\n",
    "\n",
    "# requirements\n",
    "\n",
    "1. Python programming language\n",
    "2. Pandas library\n",
    "3. Bayes's theorem\n",
    "\n",
    "# intro\n",
    "The Naive Bayes model is a machine learning model that is based on the Bayes theorem and probabilities. They are used for solving classification problems, such as determining whether an email is a spam or ham. Bayes theorem can be used to update our beliefs, this property also shows up in the model.\n",
    "\n",
    "before continuing, let us take a look at Bayes's formula\n",
    "$\\Large{P(A|B)} = \\Large\\frac{P(B|A).P(A)}{P(B)}$\n",
    "\n",
    "# how does the model work in theory\n",
    "First, let me change the variable names in the Bayes formula to better suit our case.\n",
    "\n",
    "$\\Large{P(spam|w)} = \\Large\\frac{P(w|spam)P(spam)}{P(w)} = \\Large\\frac{P(w|spam)P(spam)}{P(w|spam)P(spam) + P(w|ham)P(ham)}$\n",
    "\n",
    "$P(spam)$ denotes the probability of an input to be of class spam and $w$ denotes our feature vector.\n",
    "\n",
    "## feature vector\n",
    "Naive Bayes' method handles input as a vector of features. For instance, we may think of an apple as its features like roundness, red, crunchy, freshness, and so forth. Thus, we can define our apple as a vector.\n",
    "\n",
    "```python\n",
    "apple = ['round' , 'red' , 'crunchy' , 'fresh']\n",
    "```\n",
    "\n",
    "Now that we have the feature vector, we can calculate the probability of the apple being edible if it has these features. \n",
    "The initial problem is approachable in the same manner, only here we use the containment of words as the features. Even though we can expand more features to describe the message, I think this satisfies our needs for the moment. Check possible improvements at the end for more information.\n",
    "\n",
    "## calculating each term\n",
    "\n",
    "#### Calculating $P(spam)$\n",
    "This part is rather easy. The term expresses the probability of picking a random message and the message being a scam. Therefore we only need to assure that the data is realistic.\n",
    "\n",
    "#### Calculating $P(w|spam)$\n",
    "It turns out that calculating this part is difficult and time-consuming. To solve this challenge Naive Bayes' method assumes that the features are independent which makes our job much easier. We can use joint probability to calculate it\n",
    "\n",
    "$P(w|spam).P(spam) = P(w,spam) = P(w_{1} , w_{2} , ... , spam) = P(w_{1} | w_{2} , ... , spam).P(w_{2} , w_{3} , ... spam)$\n",
    "\n",
    "we know that our features are independent or in another way the following expression is true \n",
    "\n",
    "$\\forall{w_{a},w_{b}\\in{w}}:P(w_{a} \\cap w_{b}) = P(w_{a}).P(w_{b})$\n",
    "\n",
    "or\n",
    "\n",
    "$\\forall{w_{a},w_{b}\\in{w}}:(w_{a} | w_{b}) = P(w_{a})$\n",
    "\n",
    "this allows us to simplify our expression to\n",
    "\n",
    "$P(w|spam).P(spam) = P(w,spam) = P(w_{1}|spam)P(w_{2}|spam)...P(spam) = P(spam).\\prod_{i=0}^{n}P(w_{i}|spam)$\n",
    "\n",
    "#### Calculating $P(w_{i} | spam)$\n",
    "\n",
    "This expression means the probability of a word being in a message if we already know that its a spam. We can calculate it easily. let $N_{w_{i}|spam}$ be the number of times a word is used in a message and $N_{spam}$ the total number of words in all spam messages, then we can define $P(w_{i} | spam)$ as $\\frac{N_{w_{i}|spam}}{N_{spam}}$ this will make much more sense when we get to the implementation\n",
    "\n",
    "### note\n",
    "\n",
    "The final formula looks something like this\n",
    "\n",
    "$\\Large{P(spam|w)} = \\prod_{i=0}^{n}\\left(\\frac{P(w_{i}|spam).P(spam)}{P(w_{i}|spam)P(spam) + P(w_{i}|ham)P(ham)}\\right) $\n",
    "\n",
    "Calculating the value of each parenthesis takes four operations, and we multiply n of these therefore, we need 4n operations to calculate the probability. but because we are comparing two probabilities, we can only calculate $P(spam|w)*P(w)$ and $P(ham|w)*P(w)$ and compare these two against each other. It both mathematically makes sense. I tested both methods and found that the accuracy is not affected. Therefore we only need to calculate P(spam,w).\n",
    "\n",
    "### Special cases\n",
    "\n",
    "We now know how to calculate each term, though we haven't yet dealt with a special case the case when our word doesn't exist in the dictionary then the probability of $P(w_{i} | spam)$ is zero and this will ruin everything we can solve this problem by using [Laplace smoothing](https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece). Our final formula now looks like the following\n",
    "\n",
    "$P(w_{i} | spam) = \\Large\\frac{N_{w_{i}|spam} + a}{N_{spam} + N_{vocab}.a}$\n",
    "\n",
    "$a$ is a constant, setting it to zero signals that we are not using any smoothing. this may seem unintuitive and certainly felt like that for me but a way you can think about it is when the word is new so what we are left with is $\\frac{a}{a.N_{vocab}} $ and setting $a$ to 1 here means that the probability of that word being in a scam message is equal to we picking a random word out of the dictionary.\n",
    "\n",
    "## Pros and Cons\n",
    "\n",
    "I talked about the systematic update of beliefs earlier but some other pros include \n",
    "\n",
    "1. it allows us to think probabilistically about an object\n",
    "2. humans can easily understand it\n",
    "3. high accuracy\n",
    "\n",
    "some cons are listed below\n",
    "\n",
    "1. losing the relation between properties \n",
    "2. it needs a lot of accurate data with real word probability distribution\n",
    "\n",
    "# data\n",
    "\n",
    "I used the [SMS Spam Collection Data set](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection) from UCI.\n",
    "\n",
    "## preparing python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cb48c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\amirmahdi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\amirmahdi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\amirmahdi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\amirmahdi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\amirmahdi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: C:\\Users\\amirmahdi\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "058abfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdd52aa",
   "metadata": {},
   "source": [
    "## preparing the data\n",
    "\n",
    "We can use pandas to read the CSV file into a dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdf1ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_dataset = pd.read_csv('./data/SMSSpamCollection', sep='\\t',\n",
    "header=None, names=['Label', 'SMS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6778c6e",
   "metadata": {},
   "source": [
    "## splitting the data\n",
    "\n",
    "Now we want to split the data into a training set and a testing set 80 percent of the data will be used for training and the rest 20 percent for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50fdd577",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized_data = sms_dataset.sample(frac=1 , random_state=1)\n",
    "sepration_index = round(len(randomized_data)*0.8)\n",
    "\n",
    "training_set = sms_dataset[:sepration_index].reset_index(drop=True)\n",
    "test_set = sms_dataset[sepration_index:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ebb019",
   "metadata": {},
   "source": [
    "In this implementation we will ignore punctutatuation and words case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ee82c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After cleaning\n",
    "training_set['SMS'] = training_set['SMS'].str.replace('\\W' , ' ' , regex=True) # Removes punctuation\n",
    "training_set['SMS'] = training_set['SMS'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74d2017",
   "metadata": {},
   "source": [
    "## processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd754b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['SMS'] = training_set['SMS'].str.split()\n",
    "\n",
    "vocabulary = []\n",
    "for sms in training_set['SMS']:\n",
    "   for word in sms:\n",
    "      vocabulary.append(word)\n",
    "\n",
    "vocabulary = list(set(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243dfd77",
   "metadata": {},
   "source": [
    "We split our messages into tokens. Each token is a word. We will append all the words to a list. `set(vocabulary)` removes all the repeated words. Now we want to now the use count of a word for every message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "352a2f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_per_sms = {unique_word: [0] * len(training_set['SMS']) for unique_word in vocabulary}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8c5b1c",
   "metadata": {},
   "source": [
    "This creates a dictionary where every word is an entry and it contains a list filled with `len(training_set['SMS'])` many zeros the list represents the number of occurrences in the nth message for example `word_counts_per_sms['free'][0]` is the occurrence count for the word free in the first message. lets now fill the dictionary with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29e96786",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, sms in enumerate(training_set['SMS']):\n",
    "   for word in sms:\n",
    "      word_counts_per_sms[word][index] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5167d05",
   "metadata": {},
   "source": [
    "we can add this dictionary to our data and get a new final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d92c7497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>like</th>\n",
       "      <th>oble</th>\n",
       "      <th>evenings</th>\n",
       "      <th>division</th>\n",
       "      <th>neft</th>\n",
       "      <th>nah</th>\n",
       "      <th>lose</th>\n",
       "      <th>arranging</th>\n",
       "      <th>09061744553</th>\n",
       "      <th>bcm</th>\n",
       "      <th>...</th>\n",
       "      <th>08712460324</th>\n",
       "      <th>performance</th>\n",
       "      <th>sending</th>\n",
       "      <th>bollox</th>\n",
       "      <th>thursday</th>\n",
       "      <th>thus</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>seconds</th>\n",
       "      <th>dizzamn</th>\n",
       "      <th>5we</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7813 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   like  oble  evenings  division  neft  nah  lose  arranging  09061744553  \\\n",
       "0     0     0         0         0     0    0     0          0            0   \n",
       "1     0     0         0         0     0    0     0          0            0   \n",
       "2     0     0         0         0     0    0     0          0            0   \n",
       "3     0     0         0         0     0    0     0          0            0   \n",
       "4     0     0         0         0     0    1     0          0            0   \n",
       "\n",
       "   bcm  ...  08712460324  performance  sending  bollox  thursday  thus  \\\n",
       "0    0  ...            0            0        0       0         0     0   \n",
       "1    0  ...            0            0        0       0         0     0   \n",
       "2    0  ...            0            0        0       0         0     0   \n",
       "3    0  ...            0            0        0       0         0     0   \n",
       "4    0  ...            0            0        0       0         0     0   \n",
       "\n",
       "   0125698789  seconds  dizzamn  5we  \n",
       "0           0        0        0    0  \n",
       "1           0        0        0    0  \n",
       "2           0        0        0    0  \n",
       "3           0        0        0    0  \n",
       "4           0        0        0    0  \n",
       "\n",
       "[5 rows x 7813 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = pd.DataFrame(word_counts_per_sms)\n",
    "training_set_clean = pd.concat([training_set, word_counts], axis=1)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01e2b4d",
   "metadata": {},
   "source": [
    "We want to split the data based on the label to prevent using a filter every time we want to separate our data into spam messages and ham messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af132fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_messages = training_set_clean[training_set_clean['Label'] == 'spam']\n",
    "ham_messages = training_set_clean[training_set_clean['Label'] == 'ham']\n",
    "\n",
    "p_spam = len(spam_messages) / len(training_set_clean)\n",
    "p_ham = len(ham_messages) / len(training_set_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d27361",
   "metadata": {},
   "source": [
    "now we calculate some of the constant terms like $N_{spam}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "911c241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_spam = spam_messages['SMS'].apply(len).sum()\n",
    "n_ham = ham_messages['SMS'].apply(len).sum()\n",
    "n_vocabulary = len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c4eb7b",
   "metadata": {},
   "source": [
    "we can now have everything we need to calculate every $P(w|spam)$ if the following code seems vague go and read the [theory](#how-does-the-model-work-in-theory) again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04480a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate parameters\n",
    "parameters_spam = {unique_word:0 for unique_word in vocabulary}\n",
    "parameters_ham = {unique_word:0 for unique_word in vocabulary}\n",
    "\n",
    "# Laplace smoothing\n",
    "alpha = 1\n",
    "\n",
    "# Calculate parameters\n",
    "for word in vocabulary:\n",
    "   n_word_given_spam = spam_messages[word].sum() # spam_messages already defined\n",
    "   p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha*n_vocabulary)\n",
    "   parameters_spam[word] = p_word_given_spam\n",
    "\n",
    "   n_word_given_ham = ham_messages[word].sum() # ham_messages already defined\n",
    "   p_word_given_ham = (n_word_given_ham + alpha) / (n_ham + alpha*n_vocabulary)\n",
    "   parameters_ham[word] = p_word_given_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06485c5",
   "metadata": {},
   "source": [
    "`n_word_given_spam` as the name suggests, is the same as $N_{w|spam}$ which is the total number of occurrences in all spam messages or the probability of picking a random word out of all spam messages and it is this word. you can reason about other variables similarly. All we now need is a classify function that finishes the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca613ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(message):\n",
    "   '''\n",
    "   message: a string\n",
    "   '''\n",
    "\n",
    "   # we ignore puncutation and case\n",
    "   message = re.sub('\\W', ' ', message)\n",
    "   message = message.lower().split()\n",
    "\n",
    "   p_spam_given_message = p_spam\n",
    "   p_ham_given_message = p_ham\n",
    "\n",
    "   for word in message:\n",
    "      if word in parameters_spam:\n",
    "         p_spam_given_message *= parameters_spam[word]\n",
    "\n",
    "      if word in parameters_ham:\n",
    "         p_ham_given_message *= parameters_ham[word]\n",
    "\n",
    "   if p_ham_given_message > p_spam_given_message:\n",
    "      return 'ham'\n",
    "   elif p_spam_given_message > p_ham_given_message:\n",
    "      return 'spam'\n",
    "   else:\n",
    "      return 'needs human classification'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c64cf6",
   "metadata": {},
   "source": [
    "we can test the model using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3e8ab5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 1098\n",
      "Incorrect: 16\n",
      "Accuracy: 0.9856373429084381\n"
     ]
    }
   ],
   "source": [
    "test_set['predicted'] = test_set['SMS'].apply(classify)\n",
    "\n",
    "correct = 0\n",
    "total = test_set.shape[0]\n",
    "\n",
    "for row in test_set.iterrows():\n",
    "   row = row[1]\n",
    "   if row['Label'] == row['predicted']:\n",
    "      correct += 1\n",
    "\n",
    "print('Correct:', correct)\n",
    "print('Incorrect:', total - correct)\n",
    "print('Accuracy:', correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9657de",
   "metadata": {},
   "source": [
    "i also tested it with [another dataset](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69afea79",
   "metadata": {},
   "source": [
    "# Improvments\n",
    "\n",
    "1. store the processed data and parameters in a SQL database\n",
    "\n",
    "2. add new features to properly recognize numbers and URLs. as now numbers are not releated and just fill extra space up.\n",
    "\n",
    "3. add punctuation handling and"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
